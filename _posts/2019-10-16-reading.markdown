---
layout: post
title:  "让神经网络更深更窄"
date:   2019-10-16 20:33:01 +0800
categories: reading
---

原文链接：https://mp.weixin.qq.com/s/Zt_GJiskaVmQvr1zPDGyFA

最近的研究正在表明，深而窄的神经网络似乎更合理，麻省理工学院的Rolnick和Max Tegmark去年发表了一篇名为 The power of deeper networks for expressing natural functions 的论文，证明了通过增加深度和减少宽度，可以用“指数级”更少的神经元来执行原网络的功能。他们表明，如果你正在建模的情况有100个输入变量，那么你可以使用一层中2的100次方个神经元，或2层中2的10次方个神经元获得同样的可靠性。这两位通过一个简单的网络模拟多项式任务来证明深度的有效性，相比更浅的网络，更深的网络学习模拟多项式使用的神经元要少的多。

而同时，其他研究员一直在探索神经网络所需的最小深度，9月底，Jesse Johnson证明，在某个特定点上，再大的深度也无法弥补宽度的不足。具体来说，Johnson设计了一个分类任务， Johnson证明，当层的宽度小于或等于输入的维度时，神经网络会在这个任务中失败。
